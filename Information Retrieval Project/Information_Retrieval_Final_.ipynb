{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tSd8nx-tiyeI",
    "outputId": "fba101b8-8c4a-4631-9e8f-e60713b55f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pc4\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\pc4\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: tk in c:\\users\\pc4\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\pc4\\anaconda3\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: click in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pc4\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc4\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pc4\\anaconda3\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pc4\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk wikipedia pandas scikit-learn tk rapidfuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8RA9DsBPiyeI",
    "outputId": "322f02a7-3e2e-4d55-d418-371dbd1b309f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ttkbootstrap in c:\\users\\pc4\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: pillow>=8.2.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from ttkbootstrap) (11.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ttkbootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e6cYmu1NiyeJ",
    "outputId": "d2e71170-4dea-4579-9c9a-a9501bd01ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyQt5 in c:\\users\\pc4\\anaconda3\\lib\\site-packages (5.15.10)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\pc4\\anaconda3\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\pc4\\anaconda3\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.13 in c:\\users\\pc4\\anaconda3\\lib\\site-packages (from PyQt5) (12.13.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests->wikipedia-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests->wikipedia-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests->wikipedia-api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc4\\appdata\\roaming\\python\\python312\\site-packages (from requests->wikipedia-api) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyQt5 rapidfuzz wikipedia-api scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pc4\\anaconda3\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Pc4\\anaconda3\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "import ttkbootstrap as ttk\n",
    "from ttkbootstrap.constants import *\n",
    "from tkinter import simpledialog, messagebox\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rapidfuzz import fuzz, process\n",
    "import re\n",
    "\n",
    "\n",
    "ABBREVIATIONS = {\n",
    "    \"AI\": \"Artificial Intelligence\",\n",
    "    \"ML\": \"Machine Learning\"\n",
    "}\n",
    "\n",
    "\n",
    "PRIORITY_TOPICS = {\n",
    "    \"Artificial Intelligence\": {\n",
    "        \"aliases\": [\"AI\", \"Artificial Intelligence\"],\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "    },\n",
    "    \"Machine Learning\": {\n",
    "        \"aliases\": [\"ML\", \"Machine Learning\"],\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    },\n",
    "    \"Data Science\": {\n",
    "        \"aliases\": [\"Data Science\"],\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "    },\n",
    "    \"Data Mining\": {\n",
    "        \"aliases\": [\"Data Mining\"],\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Data_mining\"\n",
    "    },\n",
    "    \"Cloud Computing\": {\n",
    "        \"aliases\": [\"Cloud Computing\"],\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Cloud_computing\"\n",
    "    },\n",
    "    \"Cybersecurity\": {\n",
    "        \"aliases\": [\"Cybersecurity\", \"Computer Security\"],\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Computer_security\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_articles_for_topic(topic, num_articles=10):\n",
    "    search_results = wikipedia.search(topic, results=num_articles + 5)\n",
    "    articles = []\n",
    "    for title in search_results:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            articles.append({\"title\": page.title, \"url\": page.url, \"content\": page.content})\n",
    "            if len(articles) == num_articles:\n",
    "                break\n",
    "        except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):\n",
    "            continue\n",
    "    return articles\n",
    "\n",
    "\n",
    "topics = PRIORITY_TOPICS.keys()\n",
    "articles = []\n",
    "for topic in topics:\n",
    "    articles.extend(get_articles_for_topic(topic))\n",
    "articles_df = pd.DataFrame(articles)\n",
    "\n",
    "\n",
    "articles_df['is_priority'] = articles_df['title'].isin(PRIORITY_TOPICS.keys())\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_df['content'])\n",
    "\n",
    "\n",
    "def expand_abbreviations(query):\n",
    "    words = query.split()\n",
    "    return \" \".join([ABBREVIATIONS.get(word.upper(), word) for word in words])\n",
    "\n",
    "\n",
    "def correct_spelling(query):\n",
    "    suggestions = []\n",
    "    for topic, data in PRIORITY_TOPICS.items():\n",
    "        for alias in data['aliases']:\n",
    "            similarity = fuzz.ratio(query.lower(), alias.lower())\n",
    "            if similarity > 80:\n",
    "                suggestions.append((alias, similarity))\n",
    "    suggestions = sorted(suggestions, key=lambda x: -x[1])\n",
    "    if suggestions:\n",
    "        return suggestions[0][0], [s[0] for s in suggestions]\n",
    "    return query, []\n",
    "\n",
    "\n",
    "def add_priority_articles(query, results):\n",
    "    priority_articles = []\n",
    "    for topic, data in PRIORITY_TOPICS.items():\n",
    "        if query.lower() in [alias.lower() for alias in data['aliases']]:\n",
    "            priority_articles.append({\n",
    "                'title': topic,\n",
    "                'url': data['url'],\n",
    "                'content': f\"Direct match for {topic}\",\n",
    "                'similarity': 1.0\n",
    "            })\n",
    "    priority_df = pd.DataFrame(priority_articles)\n",
    "    combined_results = pd.concat([priority_df, results]).drop_duplicates(subset=['title', 'url']).reset_index(drop=True)\n",
    "    return combined_results\n",
    "\n",
    "\n",
    "def remove_near_duplicates(results):\n",
    "    unique_results = []\n",
    "    seen_contents = set()\n",
    "    seen_titles = set()\n",
    "    for _, row in results.iterrows():\n",
    "        content_hash = hash(row['content'].strip().lower())\n",
    "        title_hash = hash(row['title'].strip().lower())\n",
    "        if content_hash not in seen_contents and title_hash not in seen_titles:\n",
    "            unique_results.append(row)\n",
    "            seen_contents.add(content_hash)\n",
    "            seen_titles.add(title_hash)\n",
    "    return pd.DataFrame(unique_results)\n",
    "\n",
    "\n",
    "def search_articles(query, top_n=5, min_similarity=0.1):\n",
    "    original_query = query.strip().lower()\n",
    "    expanded_query = expand_abbreviations(original_query)\n",
    "    corrected_query, suggestions = correct_spelling(original_query)\n",
    "\n",
    "    query_to_use = corrected_query if corrected_query != original_query else expanded_query\n",
    "\n",
    "    if query.startswith('\"') and query.endswith('\"'):\n",
    "        query_to_use = query.strip('\"')\n",
    "\n",
    "    query_vector = tfidf_vectorizer.transform([query_to_use])\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    for i, (title, content) in enumerate(zip(articles_df['title'], articles_df['content'])):\n",
    "        if query_to_use.lower() in title.lower():\n",
    "            cosine_similarities[i] += 3.0\n",
    "        elif expanded_query.lower() in title.lower():\n",
    "            cosine_similarities[i] += 2.0\n",
    "        if query_to_use.lower() in content.lower():\n",
    "            cosine_similarities[i] += 1.5\n",
    "        elif expanded_query.lower() in content.lower():\n",
    "            cosine_similarities[i] += 1.0\n",
    "\n",
    "    articles_df['similarity'] = cosine_similarities\n",
    "\n",
    "    filtered_df = articles_df[articles_df['similarity'] >= min_similarity]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        return pd.DataFrame(columns=['title', 'url', 'content', 'similarity']), suggestions\n",
    "\n",
    "    sorted_df = (\n",
    "        filtered_df.sort_values(by=['is_priority', 'similarity'], ascending=[False, False])\n",
    "        .drop_duplicates(subset=['title', 'url'])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    results_with_priority = add_priority_articles(query_to_use, sorted_df)\n",
    "\n",
    "   \n",
    "    results_with_priority = remove_near_duplicates(results_with_priority)\n",
    "\n",
    "    \n",
    "    if len(results_with_priority) < top_n:\n",
    "        additional_results = sorted_df.loc[~sorted_df['title'].isin(results_with_priority['title'])]\n",
    "        results_with_priority = pd.concat([results_with_priority, additional_results]).drop_duplicates(subset=['title', 'url']).reset_index(drop=True)\n",
    "\n",
    "    return results_with_priority.head(top_n), suggestions\n",
    "\n",
    "\n",
    "def display_placeholder():\n",
    "    placeholder_text = \"\"\"\n",
    "\\n\\u2728 Welcome to the Wikipedia Search System! \\u2728\\n\n",
    "Features:\n",
    "- Abbreviation Expansion\n",
    "- Case Insensitivity\n",
    "- Exact Phrase Matching (use \"quotes\")\n",
    "- Intelligent Query Priority\n",
    "- Spelling Correction with Suggestions\n",
    "- Exact and Near-Duplicate Removal\n",
    "\n",
    "Start by entering a topic to search for Wikipedia articles.\n",
    "\"\"\"\n",
    "    results_text.delete('1.0', \"end\")\n",
    "    results_text.insert(\"1.0\", placeholder_text)\n",
    "\n",
    "\n",
    "def on_search_button_click():\n",
    "    query = simpledialog.askstring(\"Input\", \"Enter your search query:\")\n",
    "    if query:\n",
    "        results, suggestions = search_articles(query)\n",
    "\n",
    "        results_text.delete('1.0', \"end\")\n",
    "\n",
    "        if suggestions and query not in [s.lower() for s in suggestions]:\n",
    "            suggestion_text = \"Did you mean: \" + \", \".join(suggestions) + \"?\\n\\n\"\n",
    "            results_text.insert(\"1.0\", suggestion_text)\n",
    "\n",
    "        if results.empty:\n",
    "            results_text.insert(\"end\", \"No results found. Please try another search term.\\n\")\n",
    "            return\n",
    "\n",
    "        for _, row in results.iterrows():\n",
    "            title = row['title']\n",
    "            url = row['url']\n",
    "            results_text.insert(\"end\", f\"\\U0001F4D1 {title}\\n\\U0001F517 {url}\\n\\n\")\n",
    "    else:\n",
    "        messagebox.showinfo(\"Info\", \"No query entered. Please type a search query.\")\n",
    "\n",
    "\n",
    "root = ttk.Window(themename=\"darkly\")\n",
    "root.title(\"Wikipedia Search System\")\n",
    "\n",
    "label = ttk.Label(root, text=\"Wikipedia Article Search\", font=(\"Helvetica\", 20), bootstyle=INFO)\n",
    "label.pack(pady=10)\n",
    "\n",
    "search_button = ttk.Button(root, text=\"Search Articles \\U0001F50D\", bootstyle=SUCCESS, command=on_search_button_click)\n",
    "search_button.pack(pady=5)\n",
    "\n",
    "results_text = ttk.Text(root, height=20, width=80, font=(\"Consolas\", 12))\n",
    "results_text.pack(padx=10, pady=10)\n",
    "\n",
    "display_placeholder()\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
